import{_ as t,c as o,a2 as r,o as a}from"./chunks/framework.DPuwY6B9.js";const u=JSON.parse('{"title":"Homework 5 — Data from the Web (REST & Scraping)","description":"","frontmatter":{},"headers":[],"relativePath":"homework/5.md","filePath":"homework/5.md"}'),i={name:"homework/5.md"};function s(n,e,l,d,c,g){return a(),o("div",null,e[0]||(e[0]=[r('<h1 id="homework-5-—-data-from-the-web-rest-scraping" tabindex="-1">Homework 5 — Data from the Web (REST &amp; Scraping) <a class="header-anchor" href="#homework-5-—-data-from-the-web-rest-scraping" aria-label="Permalink to &quot;Homework 5 — Data from the Web (REST &amp; Scraping)&quot;">​</a></h1><h2 id="overview" tabindex="-1">Overview <a class="header-anchor" href="#overview" aria-label="Permalink to &quot;Overview&quot;">​</a></h2><p>In HW5 you will:</p><ul><li>Fetch JSON from a public <strong>REST API</strong> (Nobel Prize).</li><li>Parse nested JSON and build a <strong>word-cloud</strong> from prize <strong>motivations</strong>.</li><li><strong>Scrape</strong> a small website (Books to Scrape) across multiple pages and assemble a tidy table.</li></ul><p><strong>Tools:</strong> Use anything you like (Python/R/etc.). Your report must be readable on GitHub (<strong><code>README.md</code> or <code>.ipynb</code></strong>). Commit all code needed to reproduce results.</p><h2 id="report-requirements-markdown-or-notebook" tabindex="-1">Report Requirements (Markdown or Notebook) <a class="header-anchor" href="#report-requirements-markdown-or-notebook" aria-label="Permalink to &quot;Report Requirements (Markdown or Notebook)&quot;">​</a></h2><p>Keep it concise and reproducible:</p><ul><li><strong>Title &amp; brief intro</strong> (what you fetched/scraped and why)</li><li><strong>Sections:</strong> REST API → Web Scraping</li><li>Short <strong>methods</strong> with code blocks/cells</li><li><strong>Figures/tables</strong> with captions and clear labels</li><li><strong>Reproducibility notes</strong> (how to run; environment/requirements; where raw data is saved)</li></ul><p>Avoid committing large binary files; you may cache raw JSON/HTML as small text files.</p><h2 id="part-a-—-rest-api-nobel-prize" tabindex="-1">Part A — REST API (Nobel Prize) <a class="header-anchor" href="#part-a-—-rest-api-nobel-prize" aria-label="Permalink to &quot;Part A — REST API (Nobel Prize)&quot;">​</a></h2><p><strong>Goal:</strong> Fetch <strong>Physics</strong> prize data via the <strong>Nobel Prize API (v2.x)</strong>, extract all <strong>motivations</strong>, and visualise word frequencies with a <strong>word cloud</strong>.</p><h3 id="a1-fetch-json" tabindex="-1">A1) Fetch JSON <a class="header-anchor" href="#a1-fetch-json" aria-label="Permalink to &quot;A1) Fetch JSON&quot;">​</a></h3><ul><li>Query the Nobel API for <strong>Nobel Prizes in Physics</strong> (all available years).</li><li>Handle <strong>pagination</strong> if present (iterate until all pages are retrieved).</li><li>Save the raw response(s) to disk (e.g., <code>data/nobel_physics.json</code>) for reproducibility.</li></ul><blockquote><p>Tip: When APIs return a top-level object with lists, capture the list that contains individual prizes/entries. Record any query parameters you used (category, year range, limit, offset, etc.).</p></blockquote><h3 id="a2-parse-extract-motivations" tabindex="-1">A2) Parse &amp; extract motivations <a class="header-anchor" href="#a2-parse-extract-motivations" aria-label="Permalink to &quot;A2) Parse &amp; extract motivations&quot;">​</a></h3><ul><li>From the JSON, extract <strong>every motivation string</strong> associated with the Physics prizes (include all laureates’ motivations).</li><li>Clean text (lowercase, strip punctuation/whitespace, remove <strong>stopwords</strong>; consider stemming/lemmatisation optional).</li><li>Keep a short list of <strong>domain stopwords</strong> (e.g., “nobel”, “prize”, “physics”, “prizes”, “laureate”, “motivation”) so the cloud isn’t dominated by boilerplate.</li></ul><h3 id="a3-word-cloud" tabindex="-1">A3) Word cloud <a class="header-anchor" href="#a3-word-cloud" aria-label="Permalink to &quot;A3) Word cloud&quot;">​</a></h3><ul><li>Generate and display a <strong>word cloud</strong> from the cleaned motivations.</li><li>Include the <strong>top 20 terms</strong> with counts in a small table next to or below the cloud.</li><li>Briefly interpret (2–4 sentences): What themes recur? Any surprises?</li></ul><p><strong>Acceptance (Part A)</strong></p><ul><li>API call(s) shown with parameters; pagination handled if needed.</li><li>Motivations extracted for all Physics entries; cleaning steps stated.</li><li>Word cloud + top-terms table included, with a short interpretation.</li><li>Raw JSON cached locally and referenced in the report.</li></ul><h2 id="part-b-—-web-scraping-books-to-scrape" tabindex="-1">Part B — Web Scraping (Books to Scrape) <a class="header-anchor" href="#part-b-—-web-scraping-books-to-scrape" aria-label="Permalink to &quot;Part B — Web Scraping (Books to Scrape)&quot;">​</a></h2><p><strong>Site:</strong> <a href="https://books.toscrape.com/" target="_blank" rel="noreferrer">https://books.toscrape.com/</a><strong>Scope:</strong> <strong>First three catalogue pages</strong> (page 1–3) → <strong>20 books per page</strong> → <strong>60 rows</strong> total.</p><p><strong>Target table (exact columns):</strong></p><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>upc | title | price | rating</span></span></code></pre></div><p><strong>Definitions</strong></p><ul><li><strong>upc:</strong> Product page UPC string.</li><li><strong>title:</strong> Book title.</li><li><strong>price:</strong> Price string as shown (e.g., “£51.77”).</li><li><strong>rating:</strong> Star rating as a word (e.g., “One”, “Two”, … “Five”).</li></ul><h3 id="b1-strategy" tabindex="-1">B1) Strategy <a class="header-anchor" href="#b1-strategy" aria-label="Permalink to &quot;B1) Strategy&quot;">​</a></h3><ul><li>Start from the catalogue page 1 and follow pagination to pages 2 and 3.</li><li>For each book, follow the link to its <strong>detail page</strong> to extract <strong>UPC</strong> (and confirm price/rating if needed).</li><li>Assemble a single <strong>DataFrame/table</strong> with exactly 60 rows and the four columns above.</li></ul><h3 id="b2-robustness-etiquette" tabindex="-1">B2) Robustness &amp; etiquette <a class="header-anchor" href="#b2-robustness-etiquette" aria-label="Permalink to &quot;B2) Robustness &amp; etiquette&quot;">​</a></h3><ul><li>Set a custom <strong>User-Agent</strong> header.</li><li>Add a small <strong>delay</strong> between requests (e.g., 0.5–1.0s).</li><li>Handle <strong>unexpected HTML</strong> gracefully (missing fields → skip or record <code>NA</code> with a note).</li><li>If the site uses relative links, resolve to absolute URLs safely.</li></ul><h3 id="b3-deliverable" tabindex="-1">B3) Deliverable <a class="header-anchor" href="#b3-deliverable" aria-label="Permalink to &quot;B3) Deliverable&quot;">​</a></h3><ul><li>Show the <strong>first 5 rows</strong> as a preview and the overall <strong>row count</strong> (should be 60).</li><li>Save the final table to <code>data/books_page1-3.csv</code> (or similar).</li></ul><p><strong>Acceptance (Part B)</strong></p><ul><li>Exactly <strong>60</strong> rows with the specified columns and non-empty UPCs.</li><li>Clear method (pagination, per-book detail fetch, delays).</li><li>Clean, readable code; a brief note on any anomalies encountered.</li></ul><h2 id="submission" tabindex="-1">Submission <a class="header-anchor" href="#submission" aria-label="Permalink to &quot;Submission&quot;">​</a></h2><ol><li><p>Push your work to <strong><code>username-homework-5</code></strong>.</p></li><li><p>Open an Issue titled <strong><code>HW5 – Submission</code></strong> (optional label: <code>ready-for-grading</code>). Include:</p><ul><li>Link to your report (<code>HW5/README.md</code> <strong>or</strong> <code>HW5/HW5.ipynb</code>).</li><li>2–4 lines summarising results (API + cloud; scraper table size).</li><li>Any notes on rate limiting, pagination, or HTML quirks.</li></ul></li></ol><p><strong>Assignment deadline:</strong> <strong>Tuesday 23:59 (Europe/Stockholm)</strong></p><h2 id="peer-review-after-the-deadline" tabindex="-1">Peer Review (after the deadline) <a class="header-anchor" href="#peer-review-after-the-deadline" aria-label="Permalink to &quot;Peer Review (after the deadline)&quot;">​</a></h2><p>Comment under your partner’s <strong><code>HW5 – Submission</code></strong> Issue. Copy this checklist:</p><ul><li><strong>Coverage:</strong> API fetch + motivations + word cloud; scraper with 60 rows and required columns</li><li><strong>Reproducibility:</strong> Raw JSON cached; scraper code deterministic with delays; environment notes included</li><li><strong>Clarity:</strong> Report structure, labelled figure/table, concise explanations</li><li><strong>Correctness:</strong> All Physics motivations included; UPC/price/rating correctly captured</li><li><strong>One highlight &amp; one suggestion:</strong> Specific and actionable</li></ul><p><strong>Peer-review deadline:</strong> <strong>Thursday 23:59 (Europe/Stockholm)</strong></p><h2 id="grading" tabindex="-1">Grading <a class="header-anchor" href="#grading" aria-label="Permalink to &quot;Grading&quot;">​</a></h2><p>Per-homework scale <strong>U / G / VG</strong> based on:</p><ul><li><strong>Completeness</strong> (all tasks + submission + peer review)</li><li><strong>Clarity</strong> (clean structure; readable outputs; brief, precise writing)</li><li><strong>Correctness &amp; Reproducibility</strong> (API handling, parsing/cleaning, scraper reliability; code runs or is clearly explained)</li></ul><p><strong>Notes</strong></p><ul><li>Late submissions/reviews require an <strong>extra task</strong> and are graded <strong>Pass/Fail</strong> only (no VG).</li><li><strong>Ethics:</strong> Scrape politely (headers, delays) and stay within the stated page scope.</li><li><strong>Citations:</strong> If you adapt code (e.g., for word clouds), cite your source briefly in the report.</li></ul>',46)]))}const h=t(i,[["render",s]]);export{u as __pageData,h as default};
